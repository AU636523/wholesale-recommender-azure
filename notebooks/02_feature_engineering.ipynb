{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e19ec6",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5929a321",
   "metadata": {},
   "source": [
    "This notebook performs the feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb7705",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81033114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ETL and Data Manipulation\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, to_date, expr\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f6c8a",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1090024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LocalSparkForTesting\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "971ca21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.abspath(os.path.join('/sparkdata/wholesale-recommender', 'cleaned'))\n",
    "\n",
    "order_lines = spark.read.parquet(os.path.join(DATA_PATH, \"order_lines\"))\n",
    "products = spark.read.parquet(os.path.join(DATA_PATH, \"products\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cdd95d",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60345e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique customer base\n",
    "customers = order_lines.select(\"Customer ID\").distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c058f96",
   "metadata": {},
   "source": [
    "### Activity-based Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb745c7",
   "metadata": {},
   "source": [
    "#### Order Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd119dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_counts = order_lines.groupBy(\"Customer ID\") \\\n",
    "    .agg(F.countDistinct(\"Order ID\").alias(\"order_count\"))\n",
    "\n",
    "customers = customers.join(order_counts, on=\"Customer ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad51185",
   "metadata": {},
   "source": [
    "#### Months Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15dadf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_extremes = order_lines.groupBy(\"Customer ID\").agg(\n",
    "    F.min(\"Date Order was placed\").alias(\"first_order\"),\n",
    "    F.max(\"Date Order was placed\").alias(\"most_recent_order\")\n",
    ").withColumn(\n",
    "    \"lifetime_days\", F.datediff(\"most_recent_order\", \"first_order\")\n",
    ").withColumn(\n",
    "    \"active_months\", F.col(\"lifetime_days\") / 30\n",
    ")\n",
    "\n",
    "customers = customers.join(activity_extremes.select(\"Customer ID\", \"active_months\"), on=\"Customer ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffaeb95",
   "metadata": {},
   "source": [
    "#### Avg days between orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c508be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lifetime_days = activity_extremes.agg(F.max(\"lifetime_days\")).first()[0]\n",
    "\n",
    "customers = customers.withColumn(\n",
    "    \"avg_days_between_orders\",\n",
    "    F.when(F.col(\"order_count\") > 1, (F.col(\"active_months\") * 30) / F.col(\"order_count\"))\n",
    "     .otherwise(F.lit(max_lifetime_days))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd780b",
   "metadata": {},
   "source": [
    "### Monetary Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81257a69",
   "metadata": {},
   "source": [
    "#### Average order value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed25397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_order_value = order_lines.groupBy(\"Customer ID\") \\\n",
    "    .agg(F.mean(\"Total Cost price\").alias(\"avg_order_value\"))\n",
    "\n",
    "customers = customers.join(avg_order_value, on=\"Customer ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd584a",
   "metadata": {},
   "source": [
    "### Product Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c2c7b",
   "metadata": {},
   "source": [
    "#### # Unique Categories bought and Unique Groups bought from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54aa7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with product details if needed\n",
    "order_lines_products = order_lines.join(products, on=\"Product ID\", how=\"left\")\n",
    "\n",
    "n_unique = order_lines_products.groupBy(\"Customer ID\").agg(\n",
    "    F.countDistinct(\"Product Category\").alias(\"n_unique_categories\"),\n",
    "    F.countDistinct(\"Product Group\").alias(\"n_unique_groups\")\n",
    ")\n",
    "\n",
    "customers = customers.join(n_unique, on=\"Customer ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a917575",
   "metadata": {},
   "source": [
    "### Seasonality Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694450e5",
   "metadata": {},
   "source": [
    "#### Quarterly Distribution of orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ea88ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Add quarter column to orders\n",
    "order_lines_with_quarter = order_lines.withColumn(\"Quarter\", F.quarter(\"Delivery Date\"))\n",
    "\n",
    "# Aggregate cost per customer per quarter\n",
    "quarterly_orders = order_lines_with_quarter.groupBy(\"Customer ID\", \"Quarter\") \\\n",
    "    .agg(F.sum(\"Total Cost price\").alias(\"sum_quarter_orders\"))\n",
    "\n",
    "# Pivot to wide format: Q1, Q2, Q3, Q4\n",
    "quarterly_pivot = quarterly_orders.groupBy(\"Customer ID\") \\\n",
    "    .pivot(\"Quarter\", [1, 2, 3, 4]) \\\n",
    "    .agg(F.sum(\"sum_quarter_orders\"))\n",
    "\n",
    "# Fill missing quarters with 0\n",
    "for q in [1, 2, 3, 4]:\n",
    "    quarterly_pivot = quarterly_pivot.withColumn(str(q), F.coalesce(F.col(str(q)), F.lit(0)))\n",
    "\n",
    "# Normalize to get ratios per quarter\n",
    "total = sum([F.col(str(q)) for q in [1, 2, 3, 4]])\n",
    "for q in [1, 2, 3, 4]:\n",
    "    quarterly_pivot = quarterly_pivot.withColumn(f\"Q{q}_rate\", F.col(str(q)) / total)\n",
    "\n",
    "# Select final normalized columns\n",
    "quarterly_rates = quarterly_pivot.select(\n",
    "    \"Customer ID\", \"Q1_rate\", \"Q2_rate\", \"Q3_rate\", \"Q4_rate\"\n",
    ")\n",
    "\n",
    "# Join to customers\n",
    "customers = customers.join(quarterly_rates, on=\"Customer ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a607ac4d",
   "metadata": {},
   "source": [
    "### Purchases from categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f027dfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# 1. Join product metadata\n",
    "order_lines_products = order_lines.join(products, on=\"Product ID\", how=\"left\")\n",
    "\n",
    "# 2. Create hierarchical category column\n",
    "order_lines_products = order_lines_products.withColumn(\n",
    "    \"product_group_cat_name\",\n",
    "    concat_ws(\"_\", F.col(\"Product Line\"), F.col(\"Product Category\"))\n",
    ")\n",
    "\n",
    "# 3. Sum total spend by customer + category\n",
    "customer_orders_sum_prod_group = order_lines_products.groupBy(\"Customer ID\", \"product_group_cat_name\") \\\n",
    "    .agg(F.sum(\"Total Cost price\").alias(\"group_sum\"))\n",
    "\n",
    "# 4. Pivot to wide format: 1 column per group\n",
    "order_lines_product_groups_agg = customer_orders_sum_prod_group.groupBy(\"Customer ID\") \\\n",
    "    .pivot(\"product_group_cat_name\") \\\n",
    "    .agg(F.first(\"group_sum\"))\n",
    "\n",
    "# 5. Fill nulls with 0\n",
    "order_lines_product_groups_agg = order_lines_product_groups_agg.fillna(0)\n",
    "\n",
    "# 6. Normalize row-wise: value / row sum\n",
    "row_sum_expr = sum(F.col(c) for c in order_lines_product_groups_agg.columns if c != \"Customer ID\")\n",
    "normalized = order_lines_product_groups_agg.select(\n",
    "    \"Customer ID\",\n",
    "    *[\n",
    "        (F.col(c) / row_sum_expr).alias(f\"category_{c}\")\n",
    "        for c in order_lines_product_groups_agg.columns if c != \"Customer ID\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 7. Join back to customer\n",
    "customers = customers.join(normalized, on=\"Customer ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b68dcd",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27a82ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = os.path.abspath(os.path.join('/sparkdata/wholesale-recommender', 'processed'))\n",
    "customers.write.mode(\"overwrite\").parquet(os.path.join(OUTPUT_PATH, \"customers_features\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
