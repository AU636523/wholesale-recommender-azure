{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bb4c14",
   "metadata": {},
   "source": [
    "# Preprocess for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef72a88",
   "metadata": {},
   "source": [
    "This notebook should do the preprocessing for the training of the individual models. \n",
    "\n",
    "Note: The preprocessing for some of the models are the same, so it makes sense to do it in one flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eebe96",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfdb101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ETL and Data Manipulation\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, when, udf, log1p\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f59cd0",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b72eb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LocalSparkForTesting\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c00ab3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/sparkdata/wholesale-recommender'\n",
    "\n",
    "# Lazy load customer data \n",
    "customer_features = spark.read.parquet(os.path.join(DATA_PATH, 'processed' , \"customers_features\"))\n",
    "customer_cluster = spark.read.parquet(os.path.join(DATA_PATH, 'processed', \"customer_cluster\"))\n",
    "\n",
    "# Lazy load order lines and products\n",
    "order_lines = spark.read.parquet(os.path.join(DATA_PATH, 'cleaned', \"order_lines\"))\n",
    "products = spark.read.parquet(os.path.join(DATA_PATH, 'cleaned', \"products\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4361a9f2",
   "metadata": {},
   "source": [
    "## Preprocess Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ecf3a",
   "metadata": {},
   "source": [
    "### Clean column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fc224e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns and rename to spark-friendly names\n",
    "interactions = order_lines.select(\n",
    "    col(\"Customer ID\").alias(\"customer_id\"),\n",
    "    col(\"Product ID\").alias(\"product_id\"),\n",
    "    col(\"Quantity Ordered\").alias(\"purchase_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef5657",
   "metadata": {},
   "source": [
    "### Join product category to the order lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4142b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join product category from product dataframe\n",
    "interactions_with_category = interactions.join(\n",
    "    products.select(\n",
    "        col(\"`Product ID`\").alias('product_id'),\n",
    "        col(\"`Product Category`\")\n",
    "    ),\n",
    "    on=\"product_id\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1013c2",
   "metadata": {},
   "source": [
    "### Add rating column\n",
    "\n",
    "We use the purchase count as 'ratings' (for ALS) and perform log-transform for scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff8c46f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform log-transform\n",
    "interactions = interactions_with_category.withColumn(\"rating\", log1p(col(\"purchase_count\"))) # log1p for log-transform on value plus one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c14d39",
   "metadata": {},
   "source": [
    "### Map IDs to integer indices\n",
    "\n",
    "To accomodate PySpark ALS' requirement, that ID's should be within the integer-range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efcb459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit indexers\n",
    "customer_indexer = StringIndexer(inputCol=\"customer_id\", outputCol=\"customer_index\").fit(interactions)\n",
    "product_indexer = StringIndexer(inputCol=\"product_id\", outputCol=\"product_index\").fit(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b982ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.abspath(os.path.join('/sparkdata/wholesale-recommender', 'models'))\n",
    "\n",
    "customer_model_path = os.path.join(MODEL_PATH, \"customer_indexer_model\")\n",
    "product_model_path = os.path.join(MODEL_PATH, \"product_indexer_model\")\n",
    "\n",
    "customer_indexer.write().overwrite().save(customer_model_path)\n",
    "product_indexer.write().overwrite().save(product_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cad06196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "interactions_indexed = customer_indexer.transform(interactions)\n",
    "interactions_indexed = product_indexer.transform(interactions_indexed)\n",
    "\n",
    "# Cast\n",
    "als_input = interactions_indexed.select(\n",
    "    col(\"customer_id\"), col(\"`Product Category`\"),\n",
    "    col(\"customer_index\").cast(\"int\"),\n",
    "    col(\"product_index\").cast(\"int\"),\n",
    "    col(\"rating\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5292d5",
   "metadata": {},
   "source": [
    "### Save processed model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "149b7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.abspath(os.path.join('/sparkdata/wholesale-recommender', 'processed'))\n",
    "\n",
    "# Save as parquet\n",
    "als_input.write.mode(\"overwrite\").parquet(os.path.join(DATA_PATH, \"interactions\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
